{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Please set using GPU Tesla T4 16GB in Runtime/change runtype time at the beginning."
      ],
      "metadata": {
        "id": "aMKCMe75FNOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we leverage on Hugging face's PyTorchBenchmarkArguments for comparison, we will be running the code on Google Colab.\n",
        "\n",
        "We will be testing the benchmark using the Google Colab provided Tesla T4 GPU with 16GB capacity.\n",
        "\n",
        "Reference: https://huggingface.co/blog/reformer (accessed on 10/7/2023)"
      ],
      "metadata": {
        "id": "IKaLPR6yX3sG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59033g_I90UL",
        "outputId": "9d674ac4-dfdc-4aa8-d6a9-85405405f212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# pip installs\n",
        "!pip -qq install git+https://github.com/huggingface/transformers.git\n",
        "!pip install -qq py3nvml\n",
        "\n",
        "from transformers import ReformerConfig, BertConfig, PyTorchBenchmark, PyTorchBenchmarkArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 1**"
      ],
      "metadata": {
        "id": "kkuZv5bBbzfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we compare global self attention (that is the original transformer model setting) with reformer's memory usage.\n",
        "\n",
        "By setting lsh_attn_chunk_length = local_attn_chunk_length = 16384 so that for all input sequences smaller or equal to 16384, the model automatically switches to global self-attention, which will be the same as the transformer model's original setting."
      ],
      "metadata": {
        "id": "pwO_WmqVaoKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_global = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", lsh_attn_chunk_length=16384, local_attn_chunk_length=16384, lsh_num_chunks_before=0, local_num_chunks_before=0)\n",
        "config_LSH = ReformerConfig.from_pretrained(\"google/reformer-enwik8\")\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512, 2048, 4096, 8192, 16384, 32768], batch_sizes=[1], models=[\"Transformer\", \"Reformer_with_LSH\"])\n",
        "benchmark = PyTorchBenchmark(configs=[config_global, config_LSH], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdtJOHor92s8",
        "outputId": "cfc5113e-be93-4d7c-9308-63df48ac3c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/benchmark/benchmark_args_utils.py:136: FutureWarning: The class <class 'transformers.benchmark.benchmark_args.PyTorchBenchmarkArguments'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/benchmark/benchmark_utils.py:615: FutureWarning: The class <class 'transformers.benchmark.benchmark.PyTorchBenchmark'> is deprecated. Hugging Face Benchmarking utils are deprecated in general and it is advised to use external Benchmarking libraries  to benchmark Transformer models.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 14.75 GiB total capacity; 10.88 GiB already allocated; 3.04 GiB free; 10.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0; 14.75 GiB total capacity; 10.88 GiB already allocated; 3.04 GiB free; 10.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 14.75 GiB total capacity; 1.69 GiB already allocated; 12.16 GiB free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Doesn't fit on GPU. CUDA out of memory. Tried to allocate 16.00 GiB (GPU 0; 14.75 GiB total capacity; 1.69 GiB already allocated; 12.16 GiB free; 1.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2 / 2\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "         Transformer                 1              512            0.057     \n",
            "         Transformer                 1              2048           0.284     \n",
            "         Transformer                 1              4096           0.809     \n",
            "         Transformer                 1              8192           2.816     \n",
            "         Transformer                 1             16384            N/A      \n",
            "         Transformer                 1             32768            N/A      \n",
            "      Reformer_with_LSH              1              512            0.066     \n",
            "      Reformer_with_LSH              1              2048           0.254     \n",
            "      Reformer_with_LSH              1              4096           0.489     \n",
            "      Reformer_with_LSH              1              8192           0.977     \n",
            "      Reformer_with_LSH              1             16384           1.778     \n",
            "      Reformer_with_LSH              1             32768           3.776     \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "         Transformer                 1              512             1705     \n",
            "         Transformer                 1              2048            2267     \n",
            "         Transformer                 1              4096            3943     \n",
            "         Transformer                 1              8192            8567     \n",
            "         Transformer                 1             16384            N/A      \n",
            "         Transformer                 1             32768            N/A      \n",
            "      Reformer_with_LSH              1              512             1861     \n",
            "      Reformer_with_LSH              1              2048            2463     \n",
            "      Reformer_with_LSH              1              4096            3311     \n",
            "      Reformer_with_LSH              1              8192            4985     \n",
            "      Reformer_with_LSH              1             16384            8345     \n",
            "      Reformer_with_LSH              1             32768           15065     \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is found that without adding reformer features, the memory requirement exceed for the 16K sequence length exceed the capacity of Tesla T4 16GB GPU on Google Colab, while adding the LSH can lower the memory requirement to around 8.3GB only. This leads to significant memory efficiency. Under the Reformer model, the 16GB GPU can handle the single batch with 32K input length now."
      ],
      "metadata": {
        "id": "S8tCrvu-bjBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 2**"
      ],
      "metadata": {
        "id": "Kx5W3Qmfbm4C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we test whether adding chunking further improve memory saving efficiency in reformer model. Given the computational time involved, we will focus on testing 1 batch up to 8192 sequence only, and compare the memory result with the original transformer.\n",
        "\n",
        "Remark: The running time for below part can be long (around 50 minutes), as chunking will increase computation time."
      ],
      "metadata": {
        "id": "sMaqu7LRYx58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config_LSH_chunk = ReformerConfig.from_pretrained(\"google/reformer-enwik8\", chunk_size_feed_forward=1, num_attention_heads=2, feed_forward_size=16384)  # feed forward chunk\n",
        "benchmark_args = PyTorchBenchmarkArguments(sequence_lengths=[512, 2048, 4096, 8192], batch_sizes=[1], models=[\"Reformer_with_LSH&Chunk\"])\n",
        "benchmark = PyTorchBenchmark(configs=[config_LSH_chunk], args=benchmark_args)\n",
        "result = benchmark.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2FWBBKu-Uax",
        "outputId": "60195f3f-9381-4677-f57c-cf46547ff0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 1\n",
            "\n",
            "====================       INFERENCE - SPEED - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length     Time in s   \n",
            "--------------------------------------------------------------------------------\n",
            "   Reformer_with_LSH&Chunk           1              512            3.292     \n",
            "   Reformer_with_LSH&Chunk           1              2048           13.164    \n",
            "   Reformer_with_LSH&Chunk           1              4096           26.32     \n",
            "   Reformer_with_LSH&Chunk           1              8192           52.628    \n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "====================      INFERENCE - MEMORY - RESULT       ====================\n",
            "--------------------------------------------------------------------------------\n",
            "          Model Name             Batch Size     Seq Length    Memory in MB \n",
            "--------------------------------------------------------------------------------\n",
            "   Reformer_with_LSH&Chunk           1              512             2737     \n",
            "   Reformer_with_LSH&Chunk           1              2048            2973     \n",
            "   Reformer_with_LSH&Chunk           1              4096            3299     \n",
            "   Reformer_with_LSH&Chunk           1              8192            3983     \n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the chunking feature can further lower the memory requirement for 8K token inputs from 8GB memory requirement in the original global self-attention setting to around 3.9GB now only, although it comes at the expense of lengthing the computational time.\n"
      ],
      "metadata": {
        "id": "oN8Sw8lDdQtT"
      }
    }
  ]
}